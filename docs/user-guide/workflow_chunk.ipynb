{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow - Chunk by Chunk\n",
    "In this example, we will process McStas events chunk by chunk, panel by panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Base Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas import McStasWorkflow\n",
    "from ess.nmx.data import small_mcstas_3_sample\n",
    "from ess.nmx.types import *\n",
    "\n",
    "wf = McStasWorkflow()\n",
    "# Replace with the path to your own file\n",
    "wf[FilePath] = small_mcstas_3_sample()\n",
    "wf[MaximumCounts] = 10_000\n",
    "wf[TimeBinSteps] = 50\n",
    "wf.visualize(NMXReducedDataGroup, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Raw Data Metadata\n",
    "\n",
    "`time-of-flight` coordinate and `McStasWeight2CountScaleFactor` should not be different from chunk to chunk.\n",
    "\n",
    "Therefore we need to compute `TimeBinStep` and `McStasWeight2CoutScaleFactor` before we compute `NMXReducedData`.\n",
    "\n",
    "It can be done by `ess.reduce.streaming.StreamProcessor`.\n",
    "\n",
    "In this example, `MinimumTimeOfArrival`, `MaximumTimeOfArrival` and `MaximumProbability` will be renewed every time a chunk is added to the streaming processor.\n",
    "\n",
    "`(Min/Max)Accumulator` remembers the previous minimum/maximum value and compute new minimum/maximum value with the new chunk.\n",
    "\n",
    "``raw_event_data_chunk_generator`` yields a chunk of raw event probability from mcstas h5 file.\n",
    "\n",
    "This example below process the data chunk by chunk with size: ``CHUNK_SIZE``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.reduce.streaming import StreamProcessor, MaxAccumulator, MinAccumulator\n",
    "\n",
    "# Stream processor building helper\n",
    "scalefactor_stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventProbability,),\n",
    "    target_keys=(NMXRawDataMetadata,),\n",
    "    accumulators={\n",
    "        MaximumProbability: MaxAccumulator,\n",
    "        MaximumTimeOfArrival: MaxAccumulator,\n",
    "        MinimumTimeOfArrival: MinAccumulator,\n",
    "    },\n",
    ")\n",
    "metadata_wf = wf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_wf.visualize(NMXRawDataMetadata, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.types import DetectorName\n",
    "from ess.nmx.mcstas.load import (\n",
    "    raw_event_data_chunk_generator,\n",
    "    mcstas_weight_to_probability_scalefactor,\n",
    ")\n",
    "from ess.nmx.streaming import calculate_number_of_chunks\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "CHUNK_SIZE = 10  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3\n",
    "\n",
    "# Loop over the detectors\n",
    "file_path = metadata_wf.compute(FilePath)\n",
    "raw_data_metadatas = {}\n",
    "\n",
    "for detector_i in range(0, NUM_DETECTORS):\n",
    "    temp_wf = metadata_wf.copy()\n",
    "    temp_wf[DetectorIndex] = detector_i\n",
    "    detector_name = temp_wf.compute(DetectorName)\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        temp_wf.compute(FilePath), detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = scalefactor_stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            results = processor.add_chunk({RawEventProbability: da})\n",
    "        cur_detector_progress_bar.value += 1\n",
    "    display(results[NMXRawDataMetadata])\n",
    "    raw_data_metadatas[detector_i] = results[NMXRawDataMetadata]\n",
    "\n",
    "# We take the min/maximum values of the scale factor\n",
    "min_toa = min(dg['min_toa'] for dg in raw_data_metadatas.values())\n",
    "max_toa = max(dg['max_toa'] for dg in raw_data_metadatas.values())\n",
    "max_probability = max(dg['max_probability'] for dg in raw_data_metadatas.values())\n",
    "\n",
    "toa_bin_edges = sc.linspace(dim='t', start=min_toa, stop=max_toa, num=51)\n",
    "scale_factor = mcstas_weight_to_probability_scalefactor(\n",
    "    max_counts=wf.compute(MaximumCounts), max_probability=max_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Final Output\n",
    "\n",
    "Now with the `scale_factor: McStasWeight2CountScaleFactor`, we can compute the final output chunk by chunk.\n",
    "\n",
    "We will also compute static parameters in advance so that stream processor does not compute them every time another chunk is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas.xml import McStasInstrument\n",
    "\n",
    "final_wf = wf.copy()\n",
    "# Set the scale factor and time bin edges\n",
    "final_wf[McStasWeight2CountScaleFactor] = scale_factor\n",
    "final_wf[TimeBinSteps] = toa_bin_edges\n",
    "\n",
    "# Set the crystal rotation manually for now ...\n",
    "final_wf[CrystalRotation] = sc.vector([0, 0, 0.0], unit='deg')\n",
    "# Set static info\n",
    "final_wf[McStasInstrument] = wf.compute(McStasInstrument)\n",
    "final_wf.visualize(NMXReducedDataGroup, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.reduce.streaming import StreamProcessor, EternalAccumulator\n",
    "\n",
    "# Stream processor building helper\n",
    "final_stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventProbability,),\n",
    "    target_keys=(NMXReducedDataGroup,),\n",
    "    accumulators={NMXReducedCounts: EternalAccumulator},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.types import DetectorName\n",
    "from ess.nmx.mcstas.load import raw_event_data_chunk_generator\n",
    "from ess.nmx.streaming import calculate_number_of_chunks\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "CHUNK_SIZE = 10  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3\n",
    "final_wf[TimeBinSteps] = sc.linspace(\n",
    "    't', 0.1, 0.15, 51, unit='s'\n",
    ")  # Time bin edges can be calculated from the data\n",
    "# But streaming processor only sees the first chunk\n",
    "# So we need to set it manually before processing the first chunk\n",
    "# It is a bit cumbersome since we have to know the range of the time bins in advance\n",
    "\n",
    "# Loop over the detectors\n",
    "file_path = final_wf.compute(FilePath)\n",
    "for detector_i in range(0, NUM_DETECTORS):\n",
    "    temp_wf = final_wf.copy()\n",
    "    temp_wf[DetectorIndex] = detector_i\n",
    "    # First compute static information\n",
    "    detector_name = temp_wf.compute(DetectorName)\n",
    "    temp_wf[PixelIds] = temp_wf.compute(PixelIds)\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = final_stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            results = processor.add_chunk({RawEventProbability: da})\n",
    "        cur_detector_progress_bar.value += 1\n",
    "\n",
    "    result = results[NMXReducedDataGroup]\n",
    "    display(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmx-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

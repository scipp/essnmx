{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow - Chunk by Chunk\n",
    "In this example, we will process McStas events chunk by chunk, panel by panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Base Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas import McStasWorkflow\n",
    "from ess.nmx.data import small_mcstas_3_sample\n",
    "from ess.nmx.types import *\n",
    "\n",
    "wf = McStasWorkflow()\n",
    "# Replace with the path to your own file\n",
    "wf[FilePath] = small_mcstas_3_sample()\n",
    "wf[MaximumCounts] = 10_000\n",
    "wf[TimeBinSteps] = 50\n",
    "wf.visualize(NMXReducedDataGroup, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Raw Data Metadata\n",
    "\n",
    "`time-of-flight` coordinate and `McStasWeight2CountScaleFactor` should not be different from chunk to chunk.\n",
    "\n",
    "Therefore we need to compute `TimeBinStep` and `McStasWeight2CoutScaleFactor` before we compute `NMXReducedData`.\n",
    "\n",
    "It can be done by `ess.reduce.streaming.StreamProcessor`.\n",
    "\n",
    "In this example, `MinimumTimeOfArrival`, `MaximumTimeOfArrival` and `MaximumProbability` will be renewed every time a chunk is added to the streaming processor.\n",
    "\n",
    "`(Min/Max)Accumulator` remembers the previous minimum/maximum value and compute new minimum/maximum value with the new chunk.\n",
    "\n",
    "``raw_event_data_chunk_generator`` yields a chunk of raw event probability from mcstas h5 file.\n",
    "\n",
    "This example below process the data chunk by chunk with size: ``CHUNK_SIZE``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.reduce.streaming import StreamProcessor, MaxAccumulator, MinAccumulator\n",
    "\n",
    "# Stream processor building helper\n",
    "scalefactor_stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventProbability,),\n",
    "    target_keys=(NMXRawDataMetadata,),\n",
    "    accumulators={\n",
    "        MaximumProbability: MaxAccumulator,\n",
    "        MaximumTimeOfArrival: MaxAccumulator,\n",
    "        MinimumTimeOfArrival: MinAccumulator,\n",
    "    },\n",
    ")\n",
    "metadata_wf = wf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_wf.visualize(NMXRawDataMetadata, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.types import DetectorName\n",
    "from ess.nmx.mcstas.load import (\n",
    "    raw_event_data_chunk_generator,\n",
    "    mcstas_weight_to_probability_scalefactor,\n",
    ")\n",
    "from ess.nmx.streaming import calculate_number_of_chunks\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "CHUNK_SIZE = 10  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3\n",
    "\n",
    "# Loop over the detectors\n",
    "file_path = metadata_wf.compute(FilePath)\n",
    "raw_data_metadatas = {}\n",
    "\n",
    "for detector_i in range(0, NUM_DETECTORS):\n",
    "    temp_wf = metadata_wf.copy()\n",
    "    temp_wf[DetectorIndex] = detector_i\n",
    "    detector_name = temp_wf.compute(DetectorName)\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        temp_wf.compute(FilePath), detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = scalefactor_stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            results = processor.add_chunk({RawEventProbability: da})\n",
    "        cur_detector_progress_bar.value += 1\n",
    "    display(results[NMXRawDataMetadata])\n",
    "    raw_data_metadatas[detector_i] = results[NMXRawDataMetadata]\n",
    "\n",
    "# We take the min/maximum values of the scale factor\n",
    "min_toa = min(meta.min_toa for meta in raw_data_metadatas.values())\n",
    "max_toa = max(meta.max_toa for meta in raw_data_metadatas.values())\n",
    "max_probability = max(meta.max_probability for meta in raw_data_metadatas.values())\n",
    "\n",
    "toa_bin_edges = sc.linspace(dim='t', start=min_toa, stop=max_toa, num=51)\n",
    "scale_factor = mcstas_weight_to_probability_scalefactor(\n",
    "    max_counts=wf.compute(MaximumCounts), max_probability=max_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metadata\n",
    "\n",
    "Other metadata does not require any chunk-based computation.\n",
    "\n",
    "Therefore we export the metadata first and append detector data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Final Output\n",
    "\n",
    "Now with all the metadata, we can compute the final output chunk by chunk.\n",
    "\n",
    "We will also compute static parameters in advance so that stream processor does not compute them every time another chunk is added.\n",
    "\n",
    "We will as well export the reduced data detector by detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas.xml import McStasInstrument\n",
    "\n",
    "final_wf = wf.copy()\n",
    "# Set the scale factor and time bin edges\n",
    "final_wf[McStasWeight2CountScaleFactor] = scale_factor\n",
    "final_wf[TimeBinSteps] = toa_bin_edges\n",
    "\n",
    "# Set the crystal rotation manually for now ...\n",
    "final_wf[CrystalRotation] = sc.vector([0, 0, 0.0], unit='deg')\n",
    "# Set static info\n",
    "final_wf[McStasInstrument] = wf.compute(McStasInstrument)\n",
    "final_wf.visualize(NMXReducedDataGroup, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.nexus import NXLauetofWriter\n",
    "\n",
    "\n",
    "def temp_generator(file_path, detector_name):\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        yield da\n",
    "        cur_detector_progress_bar.value += 1\n",
    "\n",
    "\n",
    "# When a panel is added to the writer,\n",
    "# the writer will start processing the data from the generator\n",
    "# and store the results in memory\n",
    "# The writer will then write the data to the file\n",
    "# when ``save`` is called\n",
    "writer = NXLauetofWriter(\n",
    "    chunk_generator=temp_generator,\n",
    "    chunk_insert_key=RawEventProbability,\n",
    "    workflow=final_wf,\n",
    "    output_filename=\"test.h5\",\n",
    "    overwrite=True,\n",
    "    extra_meta={\"McStasWeight2CountScaleFactor\": scale_factor},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector_i in range(3):\n",
    "    display(writer.add_panel(detector_id=detector_i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmx-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

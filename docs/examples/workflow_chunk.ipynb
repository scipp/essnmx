{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Workflow\n",
    "In this example, we will process McStas events chunk by chunk, panel by panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Base Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas import McStasWorkflow\n",
    "from ess.nmx.data import small_mcstas_3_sample\n",
    "from ess.nmx.types import *\n",
    "\n",
    "wf = McStasWorkflow()\n",
    "# Replace with the path to your own file\n",
    "wf[FilePath] = small_mcstas_3_sample()\n",
    "wf[MaximumCounts] = 10_000\n",
    "wf[TimeBinSteps] = 50\n",
    "wf.visualize(NMXReducedDataGroup, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Maximum Probabiliity\n",
    "\n",
    "`McStasWeight2CountScaleFactor` should not be different from chunk to chunk.\n",
    "\n",
    "Therefore we need to compute `McStasWeight2CoutScaleFactor` before we compute `NMXReducedDataGroup`.\n",
    "\n",
    "It can be done by `ess.reduce.streaming.StreamProcessor`.\n",
    "\n",
    "In this example, `MaximumProbability` will be renewed every time a chunk is added to the streaming processor.\n",
    "\n",
    "`MaxAccumulator` remembers the previous maximum value and compute new maximum value with the new chunk.\n",
    "\n",
    "``raw_event_data_chunk_generator`` yields a chunk of raw event probability from mcstas h5 file.\n",
    "\n",
    "This example below process the data chunk by chunk with size: ``CHUNK_SIZE``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.reduce.streaming import StreamProcessor\n",
    "from ess.nmx.streaming import MaxAccumulator\n",
    "\n",
    "# Stream processor building helper\n",
    "scalefactor_stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventProbability,),\n",
    "    target_keys=(McStasWeight2CountScaleFactor,),\n",
    "    accumulators={MaximumProbability: MaxAccumulator},\n",
    ")\n",
    "scalefactor_wf = wf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalefactor_wf.visualize(McStasWeight2CountScaleFactor, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.types import DetectorName\n",
    "from ess.nmx.mcstas.load import raw_event_data_chunk_generator\n",
    "from ess.nmx.streaming import calculate_number_of_chunks\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "CHUNK_SIZE = 10  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3\n",
    "\n",
    "# Loop over the detectors\n",
    "file_path = scalefactor_wf.compute(FilePath)\n",
    "scale_factors = {}\n",
    "for detector_i in range(0, NUM_DETECTORS):\n",
    "    temp_wf = scalefactor_wf.copy()\n",
    "    temp_wf[DetectorIndex] = detector_i\n",
    "    detector_name = temp_wf.compute(DetectorName)\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        temp_wf.compute(FilePath), detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = scalefactor_stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            results = processor.add_chunk({RawEventProbability: da})\n",
    "        cur_detector_progress_bar.value += 1\n",
    "    scale_factors[detector_i] = results[McStasWeight2CountScaleFactor]\n",
    "\n",
    "# We take the minimum scale factor for the entire dataset\n",
    "scale_factor = sc.concat([*scale_factors.values()], dim='panel').min()\n",
    "scale_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Final Output\n",
    "\n",
    "Now with the `scale_factor: McStasWeight2CountScaleFactor`, we can compute the final output chunk by chunk.\n",
    "\n",
    "We will also compute static parameters in advance so that stream processor does not compute them every time another chunk is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas.xml import McStasInstrument\n",
    "\n",
    "final_wf = wf.copy()\n",
    "# Add the scale factor to the workflow\n",
    "final_wf[McStasWeight2CountScaleFactor] = scale_factor\n",
    "\n",
    "# Compute the static information in advance\n",
    "# static_info = wf.compute([CrystalRotation, McStasInstrument])\n",
    "static_info = wf.compute([McStasInstrument])\n",
    "# final_wf[CrystalRotation] = static_info[CrystalRotation]\n",
    "final_wf[CrystalRotation] = sc.vector([0, 0, 0.,], unit='deg')\n",
    "final_wf[McStasInstrument] = static_info[McStasInstrument]\n",
    "final_wf.visualize(NMXReducedDataGroup, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.reduce.streaming import StreamProcessor, EternalAccumulator\n",
    "\n",
    "# Stream processor building helper\n",
    "final_stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventProbability,),\n",
    "    target_keys=(NMXReducedDataGroup,),\n",
    "    accumulators={NMXReducedCounts: EternalAccumulator},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.types import DetectorName\n",
    "from ess.nmx.mcstas.load import raw_event_data_chunk_generator\n",
    "from ess.nmx.streaming import calculate_number_of_chunks\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "CHUNK_SIZE = 10  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3\n",
    "final_wf[TimeBinSteps] = sc.linspace(\n",
    "    't', 0.1, 0.15, 51, unit='s'\n",
    ")  # Time bin edges can be calculated from the data\n",
    "# But streaming processor only sees the first chunk\n",
    "# So we need to set it manually before processing the first chunk\n",
    "# It is a bit cumbersome since we have to know the range of the time bins in advance\n",
    "\n",
    "# Loop over the detectors\n",
    "file_path = final_wf.compute(FilePath)\n",
    "for detector_i in range(0, NUM_DETECTORS):\n",
    "    temp_wf = final_wf.copy()\n",
    "    temp_wf[DetectorIndex] = detector_i\n",
    "    # First compute static information\n",
    "    detector_name = temp_wf.compute(DetectorName)\n",
    "    temp_wf[PixelIds] = temp_wf.compute(PixelIds)\n",
    "    max_chunk_id = calculate_number_of_chunks(\n",
    "        file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    cur_detector_progress_bar = IntProgress(\n",
    "        min=0, max=max_chunk_id, description=f\"Detector {detector_i}\"\n",
    "    )\n",
    "    display(cur_detector_progress_bar)\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = final_stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunk_generator(\n",
    "        file_path=file_path, detector_name=detector_name, chunk_size=CHUNK_SIZE\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            results = processor.add_chunk({RawEventProbability: da})\n",
    "        cur_detector_progress_bar.value += 1\n",
    "\n",
    "    result = results[NMXReducedDataGroup]\n",
    "    display(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmx-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Chunk Workflow\n",
    "In this example, we will process McStas events chunk by chunk, panel by panel.\n",
    "\n",
    "This example is to support processing big size files.\n",
    "\n",
    "See ``workflow`` example about the workflow in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Base Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas import McStasWorkflow\n",
    "from ess.nmx.data import small_mcstas_3_sample\n",
    "from ess.nmx.reduction import NMXReducedData\n",
    "from ess.nmx.types import *\n",
    "\n",
    "wf = McStasWorkflow()\n",
    "# Replace with the path to your own file\n",
    "wf[FilePath] = small_mcstas_3_sample()\n",
    "wf[MaximumCounts] = 10_000\n",
    "wf[TimeBinSteps] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.visualize(NMXReducedData, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Processor and Accumulator\n",
    "\n",
    "``StreamingProcessor`` allows processing data chunk by chunk.\n",
    "\n",
    "``Accumulator`` defines how to keep track of the latest result of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.reduce.streaming import EternalAccumulator, Accumulator\n",
    "from ess.reduce.streaming import StreamProcessor\n",
    "\n",
    "\n",
    "class MaxAccumulator(Accumulator):\n",
    "    \"\"\"Accumulator that keeps track of the maximum value seen so far.\"\"\"\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._cur_max: sc.Variable | None = None\n",
    "\n",
    "    @property\n",
    "    def value(self) -> sc.Variable | None:\n",
    "        return self._cur_max\n",
    "\n",
    "    def _do_push(self, value: sc.Variable) -> None:\n",
    "        new_max = value.max()\n",
    "        if self._cur_max is None:\n",
    "            self._cur_max = new_max\n",
    "        else:\n",
    "            self._cur_max = sc.concat([self._cur_max, new_max], dim='max').max('max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ess.nmx.types import RawEventData, ProtonCharge, NMXHistogram, MaximumProbability\n",
    "\n",
    "# Stream processor building helper\n",
    "stream_processor = partial(\n",
    "    StreamProcessor,\n",
    "    dynamic_keys=(RawEventData,),\n",
    "    target_keys=(ProtonCharge, NMXHistogram),\n",
    "    accumulators={\n",
    "        ProtonCharge: EternalAccumulator,\n",
    "        NMXHistogram: EternalAccumulator,\n",
    "        MaximumProbability: MaxAccumulator,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Chunk by Chunk and Export Panel by Panel\n",
    "\n",
    "Since it is for saving memories, exporting reduced data for each panel cannot be separated from the processing loop.\n",
    "\n",
    "This example below process the data chunk by chunk with size: ``CHUNK_SIZE``\n",
    "\n",
    "and export the data into ``FINE_NAME`` panel by panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciline as sl\n",
    "from contextlib import contextmanager\n",
    "from collections.abc import Generator\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def temp_parameter(\n",
    "    wf: sl.Pipeline, parameter_type: type, value: Any\n",
    ") -> Generator[sl.Pipeline]:\n",
    "    \"\"\"Helper to temporarily change a parameter in a pipeline.\"\"\"\n",
    "    copied = wf.copy()\n",
    "    copied[parameter_type] = value\n",
    "    yield copied\n",
    "    del copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from ess.nmx.types import DetectorBankPrefix, DetectorName, CrystalRotation\n",
    "from ess.nmx.mcstas.load import raw_event_data_chunks, McStasInstrument\n",
    "from ess.nmx.reduction import format_nmx_reduced_data\n",
    "from ess.nmx.nexus import (\n",
    "    export_panel_dependent_data_as_nxlauetof,\n",
    "    export_panel_independent_data_as_nxlauetof,\n",
    ")\n",
    "\n",
    "FINE_NAME = \"test.h5\"\n",
    "CHUNK_SIZE = 100  # Number of event rows to process at once\n",
    "# Increase this number to speed up the processing\n",
    "NUM_DETECTORS = 3  # Normally 3, but we only have 1 in the small sample\n",
    "\n",
    "# Loop over the detectors\n",
    "for i in range(0, NUM_DETECTORS):\n",
    "    # Set the detector index and compute the static results first\n",
    "    with temp_parameter(wf, DetectorIndex, i) as temp_wf:\n",
    "        static_results = temp_wf.compute(\n",
    "            [DetectorName, DetectorBankPrefix, CrystalRotation, McStasInstrument]\n",
    "        )\n",
    "    mcstas_geo: McStasInstrument = static_results[McStasInstrument]\n",
    "    prefix: DetectorBankPrefix = static_results[DetectorBankPrefix]\n",
    "    detector_name: DetectorName = static_results[DetectorName]\n",
    "    crystal_rotation: CrystalRotation = static_results[CrystalRotation]\n",
    "    pixel_ids = mcstas_geo.to_coords(detector_name).pop(\"pixel_id\")\n",
    "\n",
    "    # Build the stream processor\n",
    "    processor = stream_processor(temp_wf)\n",
    "    for da in raw_event_data_chunks(\n",
    "        file_path=FilePath(small_mcstas_3_sample()),\n",
    "        bank_prefix=prefix,\n",
    "        pixel_ids=pixel_ids,\n",
    "        chunk_size=100,\n",
    "    ):\n",
    "        if any(da.sizes.values()) == 0:\n",
    "            continue\n",
    "        results = processor.add_chunk({RawEventData: da})\n",
    "\n",
    "    proton_charge, histogram = results[ProtonCharge], results[NMXHistogram]\n",
    "    reduced = format_nmx_reduced_data(\n",
    "        counts=histogram,\n",
    "        proton_charge=proton_charge,\n",
    "        detector_name=detector_name,\n",
    "        instrument=mcstas_geo,\n",
    "        crystal_rotation=crystal_rotation,\n",
    "    )\n",
    "    # Replace the time bin steps with the first set of time bins\n",
    "    # This might exclude some time bins if the first set is not complete\n",
    "    # Better to set it manually as a parameter but it is not possible\n",
    "    # to enforce the user to set it as an array when using the streaming processor\n",
    "    wf[TimeBinSteps] = reduced['counts'].coords['t']\n",
    "    # Export each panel to the same file\n",
    "    if not pathlib.Path(FINE_NAME).exists() or i == 0:\n",
    "        export_panel_independent_data_as_nxlauetof(reduced, output_file=FINE_NAME)\n",
    "    export_panel_dependent_data_as_nxlauetof(reduced, output_file=FINE_NAME)\n",
    "    del reduced\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmx-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

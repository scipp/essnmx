{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "In this example, we will use McStas 3 simulation file.\n",
    "\n",
    "## Build Pipeline (Collect Parameters and Providers)\n",
    "Import the providers from ``load_mcstas_nexus`` to use the ``McStas`` simulation data workflow. <br>\n",
    "``MaximumProbability`` can be manually provided to derive more realistic number of events. <br>\n",
    "It is because ``weights`` are given as probability, not number of events in a McStas file. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.mcstas import McStasWorkflow\n",
    "from ess.nmx.data import small_mcstas_3_sample\n",
    "\n",
    "from ess.nmx.types import *\n",
    "from ess.nmx.reduction import NMXData, NMXReducedData, merge_panels\n",
    "from ess.nmx.types import DetectorIndex\n",
    "\n",
    "\n",
    "wf = McStasWorkflow()\n",
    "# Replace with the path to your own file\n",
    "wf[FilePath] = small_mcstas_3_sample()\n",
    "wf[MaximumProbability] = 10000\n",
    "wf[TimeBinSteps] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the workflow can produce, display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reduce all three panels, so we map the relevant part of the workflow over a list of the three panels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reduce all three panels,\n",
    "map the relevant part of the workflow over a list of the three panels:\n",
    "\n",
    "```python\n",
    "# DetectorIndex selects what detector panels to include in the run\n",
    "# in this case we select all three panels.\n",
    "wf[NMXReducedData] = (\n",
    "    wf[NMXReducedData]\n",
    "    .map({DetectorIndex: sc.arange('panel', 3, unit=None)})\n",
    "    .reduce(index=\"panel\", func=merge_panels)\n",
    ")\n",
    "```\n",
    "\n",
    "However, we encountered memory issue processing dataset, which is often over 10GB.\n",
    "Therefore we will not merge the panels at the end of the workflow\n",
    "and iter over the detector index instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.visualize(NMXReducedData, graph_attr={\"rankdir\": \"TD\"}, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Desired Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciline as sl\n",
    "from contextlib import contextmanager\n",
    "from collections.abc import Generator\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def temp_parameter(\n",
    "    wf: sl.Pipeline, parameter_type: type, value: Any\n",
    ") -> Generator[sl.Pipeline]:\n",
    "    copied = wf.copy()\n",
    "    copied[parameter_type] = value\n",
    "    yield copied\n",
    "    del copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the first detector binned by panel, pixel and timeslice\n",
    "with temp_parameter(wf, DetectorIndex, 0) as temp_wf:\n",
    "    binned_dg = temp_wf.compute(NMXReducedData)\n",
    "\n",
    "binned_dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "``NMXReducedData`` object has a method to export the data into nexus or h5 file.\n",
    "\n",
    "You can save the result as ``test.nxs``, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.nmx.nexus import export_as_nxlauetof\n",
    "\n",
    "dgs = []\n",
    "for i in range(3):\n",
    "    with temp_parameter(wf, DetectorIndex, i) as temp_wf:\n",
    "        reduced_data = temp_wf.compute(NMXReducedData)\n",
    "        dgs.append(reduced_data)\n",
    "        del reduced_data\n",
    "\n",
    "export_as_nxlauetof(*dgs, output_file=\"test.nxs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy version of the exporting method\n",
    "```python\n",
    "from ess.nmx.nexus import export_as_nexus\n",
    "\n",
    "export_as_nexus(binned_dg, \"test.nxs\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All Panels\n",
    "\n",
    "If you simply want to compute all panels at once, you can use map/reduce on the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_wf = wf.copy()\n",
    "detector_panel_ids = {DetectorIndex: sc.arange('panel', 3, unit=None)}\n",
    "pipeline = base_wf.map(detector_panel_ids)\n",
    "pipeline.visualize(\n",
    "    sl.get_mapped_node_names(pipeline, NMXData),\n",
    "    compact=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = merge_panels(\n",
    "    *pipeline.compute(sl.get_mapped_node_names(pipeline, NMXData)).values()\n",
    ")\n",
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument View\n",
    "\n",
    "Pixel positions are not used for later steps,\n",
    "but it is included in the coordinates for instrument view.\n",
    "\n",
    "All pixel positions are relative to the sample position,\n",
    "therefore the sample is at (0, 0, 0).\n",
    "\n",
    "**It might be very slow or not work in the ``VS Code`` jupyter notebook editor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scippneutron as scn\n",
    "\n",
    "da = dg[\"weights\"]\n",
    "da.coords[\"position\"] = dg[\"position\"]\n",
    "# Plot one out of 100 pixels to reduce size of docs output\n",
    "view = scn.instrument_view(da[\"id\", ::100].hist(), pixel_size=0.0075)\n",
    "view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmx-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
